{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E9%80%B2%E8%A1%8C%E9%A0%90%E6%B8%AC%E4%B8%A6%E6%8F%90%E4%BA%A4%E7%B5%90%E6%9E%9C_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import jieba.posseg as pseg\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數設定\n",
    "TRAIN_DATA = pd.read_csv('train.csv', index_col=0)\n",
    "TEST_DATA = pd.read_csv('test.csv', index_col=0)\n",
    "\n",
    "# 序列最長的長度\n",
    "MAX_SEQUENCE_LENGTH = 27\n",
    "\n",
    "# 在語料庫裡有多少詞彙\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 128\n",
    "\n",
    "# Train/Valid ratio\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# 基本參數設置，有幾個分類\n",
    "NUM_CLASSES = 30\n",
    "\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Jieba 新聞標題切割成一個個有意義的詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petition Wants A Statue Of Missy Elliott To Re...</td>\n",
       "      <td>\"Together we can put white supremacy down, fli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Law School Created A Criminal Justice Cla...</td>\n",
       "      <td>Long live Omar's code!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mom Of Racist-Ranting Alabama Student Says She...</td>\n",
       "      <td>Sorority sister Harley Barber was expelled aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline   \n",
       "id                                                      \n",
       "0   Petition Wants A Statue Of Missy Elliott To Re...  \\\n",
       "1   This Law School Created A Criminal Justice Cla...   \n",
       "2   Mom Of Racist-Ranting Alabama Student Says She...   \n",
       "\n",
       "                                    short_description  \n",
       "id                                                     \n",
       "0   \"Together we can put white supremacy down, fli...  \n",
       "1                              Long live Omar's code!  \n",
       "2   Sorority sister Harley Barber was expelled aft...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA.iloc[:, [0, 1]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1(TRAIN_DATA):\n",
    "\n",
    "    TRAIN_DATA['headline_tokenized'] = TRAIN_DATA.loc[:, 'headline'].apply(remove_between_square_brackets, remove_special_characters)\n",
    "    TRAIN_DATA['headline_tokenized'] = TRAIN_DATA['headline_tokenized'].apply(simple_stemmer, remove_stopwords)\n",
    "    TRAIN_DATA['headline_tokenized'] = TRAIN_DATA['headline_tokenized'].apply(jieba_tokenizer)\n",
    "\n",
    "    return TRAIN_DATA['headline_tokenized']\n",
    "\n",
    "def task2(TRAIN_DATA):\n",
    "\n",
    "    TRAIN_DATA['short_description_tokenized'] = TRAIN_DATA.loc[:, 'short_description'].apply(remove_between_square_brackets, remove_special_characters)\n",
    "    TRAIN_DATA['short_description_tokenized'] = TRAIN_DATA['short_description_tokenized'].apply(simple_stemmer, remove_stopwords)\n",
    "    TRAIN_DATA['short_description_tokenized'] = TRAIN_DATA['short_description_tokenized'].apply(jieba_tokenizer)\n",
    "\n",
    "    return TRAIN_DATA['short_description_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liu76\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Exception in thread Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\liu76\\.conda\\envs\\tensorflow\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\liu76\\.conda\\envs\\tensorflow\\lib\\threading.py\", line 953, in run\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\liu76\\.conda\\envs\\tensorflow\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: 'Series' object is not callable\n",
      "    self.run()\n",
      "  File \"c:\\Users\\liu76\\.conda\\envs\\tensorflow\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: 'Series' object is not callable\n"
     ]
    }
   ],
   "source": [
    "a = threading.Thread(target=task1(TRAIN_DATA))  # 建立新的執行緒\n",
    "b = threading.Thread(target=task2(TRAIN_DATA))  # 建立新的執行緒\n",
    "\n",
    "a.start()\n",
    "b.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_tokenized</th>\n",
       "      <th>short_description_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petit want statu of missi elliott to replac co...</td>\n",
       "      <td>togeth we can put white supremaci down flip it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thi law school creat crimin justic class base ...</td>\n",
       "      <td>long live omar code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom of racist alabama student say she didn rai...</td>\n",
       "      <td>soror sister harley barber wa expel after vile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   headline_tokenized   \n",
       "id                                                      \n",
       "0   petit want statu of missi elliott to replac co...  \\\n",
       "1   thi law school creat crimin justic class base ...   \n",
       "2   mom of racist alabama student say she didn rai...   \n",
       "\n",
       "                          short_description_tokenized  \n",
       "id                                                     \n",
       "0   togeth we can put white supremaci down flip it...  \n",
       "1                                 long live omar code  \n",
       "2   soror sister harley barber wa expel after vile...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA.iloc[:, [3, 4]].head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字典並將文本轉成數字序列\n",
    "\n",
    "可以分 4 個步驟將手上的新聞標題全部轉為數字序列：\n",
    "\n",
    "1. 將已被斷詞的新聞標題 A 以及新聞標題 B 全部倒在一起\n",
    "2. 建立一個空字典\n",
    "3. 查看所有新聞標題，裏頭每出現一個字典裡頭沒有的詞彙，就為該詞彙指定一個字典裡頭還沒出現的索引數字，並將該詞彙放入字典\n",
    "4. 利用建好的字典，將每個新聞標題裡頭包含的詞彙轉換成數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將一段文字轉換成一系列的詞彙（Tokens），並為其建立字典。\n",
    "# 這邊的 num_words=20000 代表我們限制字典只能包含 20,000 個詞彙\n",
    "# 一旦字典達到這個大小以後，剩餘的新詞彙都會被視為 Unknown，以避免字典過於龐大。\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_x1 = TRAIN_DATA.headline_tokenized\n",
    "corpus_x2 = TRAIN_DATA.short_description_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "\n",
    "pd.DataFrame(corpus.iloc[:5], columns=['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了語料庫以後，接下來就是呼叫 tokenizer 為我們查看所有文本，並建立一個字典（步驟 2 & 3）：\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行上述第 4 個步驟，請 tokenizer 利用內部生成的字典分別將我們的新聞標題 A 與 新聞 B 轉換成數字序列：\n",
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 tokenizer.index_word 來將索引數字對應回本來的詞彙：\n",
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列的 Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雖然我們已經將每個新聞標題的文本轉為一行行的數字序列，你會發現每篇標題的序列長度並不相同：\n",
    "for seq in x1_train[:5]:\n",
    "    print(len(seq), seq[:5], ' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(seq) for seq in x1_train])\n",
    "print(\"最長的序列甚至達到 \" + str(max_seq_len) + \" 個詞彙!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長度超過此數字的序列尾巴會被刪掉；而針對原來長度不足的序列，我們則會在詞彙前面補零。\n",
    "x1_train = pad_sequences(x1_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_train = pad_sequences(x2_train, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 27\n",
    "    \n",
    "print(\"所有新聞標題的序列長度皆為 27 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將正解做 One-hot Encoding (處理Label的部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'BLACK VOICES':0, \n",
    "    'BUSINESS':1, \n",
    "    'WOMEN':2, \n",
    "    'WELLNESS':3, \n",
    "    'WEIRD NEWS':4, \n",
    "    'WEDDINGS':5, \n",
    "    'TRAVEL':6, \n",
    "    'THE WORLDPOST':7, \n",
    "    'TECH':8, \n",
    "    'TASTE':9,\n",
    "    'STYLE & BEAUTY':10, \n",
    "    'STYLE':11, \n",
    "    'SPORTS':12, \n",
    "    'SCIENCE':13, \n",
    "    'RELIGION':14, \n",
    "    'QUEER VOICES':15, \n",
    "    'POLITICS':16, \n",
    "    'PARENTS':17, \n",
    "    'PARENTING':18, \n",
    "    'MEDIA':19,\n",
    "    'IMPACT':20, \n",
    "    'HOME & LIVING':21, \n",
    "    'HEALTHY LIVING':22, \n",
    "    'GREEN':23, \n",
    "    'FOOD & DRINK':24, \n",
    "    'ENTERTAINMENT':25, \n",
    "    'DIVORCE':26, \n",
    "    'CRIME':27, \n",
    "    'COMEDY':28, \n",
    "    'WORLD NEWS':29\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = TRAIN_DATA.category.apply(lambda x: label_to_index[x])\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_train[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切割訓練資料集 & 驗證資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = train_test_split(\n",
    "        x1_train, x2_train, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(x1_train[:5]):\n",
    "    print(f\"新聞標題 {i + 1}: \")\n",
    "    print([tokenizer.index_word.get(idx, '') for idx in seq])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Simple_LSTM, CNN_1D, GRU_model\n",
    "\n",
    "model = Simple_LSTM(MAX_NUM_WORDS, NUM_EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, NUM_LSTM_UNITS, NUM_CLASSES)\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 60\n",
    "\n",
    "checkpoint = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], \n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=([x1_val, x2_val], y_val),\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''merged_x1 = np.vstack([x1_train, x1_val])\n",
    "merged_x2 = np.vstack([x2_train, x2_val])\n",
    "merged_x = np.hstack([merged_x1, merged_x2])\n",
    "\n",
    "merged_y = np.vstack([y_train, y_val])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for kfold, (train, valid) in enumerate(KFold(n_splits=3, shuffle=True).split(merged_x, merged_y)):\n",
    "    # clear the session \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    print('------------------------------------------')\n",
    "    print('            Fold ' + str(kfold+1) + ' processing')\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "    # run the model \n",
    "    history = model.fit(merged_x[train], merged_y[train], \n",
    "                        batch_size=512, \n",
    "                        epochs=100, \n",
    "                        validation_data=(merged_x[valid], merged_y[valid]), \n",
    "                        callbacks=[checkpoint],\n",
    "                        shuffle=True, \n",
    "                        verbose=1)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION = pd.read_csv('sample_submission.csv', index_col=0)\n",
    "SUBMISSION.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下步驟分別對新聞標題 A、B　進行\n",
    "# 文本斷詞 / Word Segmentation\n",
    "TEST_DATA['headline_tokenized'] = TEST_DATA.loc[:, 'headline'].apply(remove_between_square_brackets, remove_special_characters)\n",
    "TEST_DATA['headline_tokenized'] = TEST_DATA['headline_tokenized'].apply(simple_stemmer, remove_stopwords)\n",
    "TEST_DATA['headline_tokenized'] = TEST_DATA['headline_tokenized'].apply(jieba_tokenizer)\n",
    "\n",
    "TEST_DATA['short_description_tokenized'] = TEST_DATA.loc[:, 'short_description'].apply(remove_between_square_brackets, remove_special_characters)\n",
    "TEST_DATA['short_description_tokenized'] = TEST_DATA['short_description_tokenized'].apply(simple_stemmer, remove_stopwords)\n",
    "TEST_DATA['short_description_tokenized'] = TEST_DATA['short_description_tokenized'].apply(jieba_tokenizer)\n",
    "\n",
    "# 將詞彙序列轉為索引數字的序列\n",
    "x1_test = tokenizer.texts_to_sequences(TEST_DATA.headline_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(TEST_DATA.short_description_tokenized)\n",
    "\n",
    "# 為數字序列加入 zero padding\n",
    "x1_test = pad_sequences(x1_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_test = pad_sequences(x2_test, maxlen=MAX_SEQUENCE_LENGTH)    \n",
    "\n",
    "# 利用已訓練的模型做預測\n",
    "merged_x_test = np.hstack([x1_test, x2_test])\n",
    "predictions = model.predict(merged_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "TEST_DATA['category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "\n",
    "submission = TEST_DATA.loc[:, ['category']].reset_index()\n",
    "\n",
    "submission.columns = ['id', 'category']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一次提交結果以後，我們還可以做非常多事情來嘗試改善模型效能：\n",
    "\n",
    "1. 改變字典詞彙量、序列長度\n",
    "2. 改變詞向量的維度\n",
    "3. 嘗試預先訓練的詞向量如 ELMo、GloVe\n",
    "4. 調整 LSTM 層的輸出維度\n",
    "5. 使用不同優化器、調整 Learning rate\n",
    "6. 改變神經網路架構如使用 GRU 層\n",
    "...\n",
    "能改善準確度的方式不少，但因為牽涉範圍太廣，請容許我把它們留給你當做回家作業。\n",
    "\n",
    "走到這裡代表你已經完整地經歷了一個 NLP 專案所需要的大部分步驟。在下一節．讓我們回顧一下在這趟旅程中你所學到的東西。\n",
    "\n",
    "我們是怎麼走到這裡的\n",
    "在這趟 NLP 旅程裏頭，我們學會了不少東西。\n",
    "\n",
    "現在的你應該已經了解：\n",
    "\n",
    " - NLP 中常見的數據前處理以及實踐方法\n",
    " - 詞向量以及詞嵌入的基本概念\n",
    " - 神經網路常見的元件如全連接層、簡單 RNN 以及 LSTM\n",
    " - 能讀多個資料來源的孿生神經網路架構\n",
    " - 如何用 Keras 建構一個完整的神經網路\n",
    " - 深度學習 3 步驟：建模、定義衡量指標以及訓練模型\n",
    " - 梯度下降、優化器以及交叉熵等基本概念\n",
    " - 如何利用已訓練模型對新數據做預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('submission.zip', mode='w') as zf:\n",
    "    zf.write(\"submission.csv\", compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud for positive review words\n",
    "plt.figure(figsize=(10,10))\n",
    "WC = WordCloud(width=1000 , height=500 , max_words=500 , min_font_size=5)\n",
    "positive_words = WC.generate(TRAIN_DATA.to_string())\n",
    "plt.imshow(positive_words, interpolation='bilinear')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
